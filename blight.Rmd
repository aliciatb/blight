---
title: "Blight Prediction"
author: "Alicia Brown"
date: "May 8, 2016"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(GGally)
require(scales)
require(caret)
require(randomForest)
```
## Overview
The purpose of this project was to determine if any data features of a city can be used to predict blight for a given location. All code, ETL workflow and analysis is available on https://github.com/aliciatb/blight.

## The data
The data provided for this project include 311 Service Calls, Crime Incidents, Blight Violations and Permits for Demolition. The first 3 datasets provide the foundation for a Buildings dataset that consists of unique locations as well as the source of derived features used in the model creation like *Number of Crimes*, *Number of Blight Violations*, and *Number of Service Calls*. These datasets were downloaded from the [course site](https://www.coursera.org/learn/datasci-capstone/supplement/D44tm/get-the-data), but are also available via [capstone project repo](https://github.com/uwescience/datasci_course_materials/tree/master/capstone/blight/data) on github. All of the data comes from [Socrata](https://www.socrata.com/) powered Detroit Open Data Portal, https://data.detroitmi.gov/.

### Files
* [Blight Violations](https://d18ky98rnyall9.cloudfront.net/_97bd1c1e5df9537bb13398c9898deed7_detroit-blight-violations.csv?Expires=1462320000&Signature=KjJzlAwVQBOONT-2ZJN7ixzhYeD~Cb1T5t4G5pIn1Alf3F7c0MTwnnYfstgr-hxGH12A9T4mayhz7uPl2zPYVk5VOIfHrkmTNiwudNJbZ0gtMjFXr~q7EFQSfi3nafc~W0sDZKezGVCVZCrPqN2RUddWIJfuli0erB1kvRNC75k_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)
* [311 Service Calls](https://d18ky98rnyall9.cloudfront.net/_dcebfb2135a2bf5a6392493bd61aba22_detroit-311.csv?Expires=1462320000&Signature=lfmBO8JTr0lHrxA-DYDkl~TfwaM6hEyPsqhhtnE1iKfEEoxKmHT62VwnJvnjccUcfrsdMfyz7YpFz-OvtXMVJBC4~d8mDPcLo~15nLr198gUHCpykWk2uV1nOln4kCQuSDvuusQDR9UMDSCAURf-I8lCM7LU3jy3IYOd73uY-HU_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)
* [Crime Incidents](https://d18ky98rnyall9.cloudfront.net/_dcebfb2135a2bf5a6392493bd61aba22_detroit-crime.csv?Expires=1462320000&Signature=FsI7KVjPOUR8ujpVKtMWouTgUmc0XY8RS2J5EjJa9Z-Yab61WBPBOroVrwoGa4UtAB9uDB2IJTVXzUx4LFz-zBEgGyd4BX4uZlbnnLkv82wW3FzJZcpMzKbpjfq0xtt4AY7DcRx69GzGl84EE4is~C5hoOIVThMcTKaALabpwW4_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)
* [Demolition Permits](https://d18ky98rnyall9.cloudfront.net/_dcebfb2135a2bf5a6392493bd61aba22_detroit-demolition-permits.tsv?Expires=1462320000&Signature=DeM232D0rs-DrPVf23KpOpGRsVyBLA6Xatck93XuCVxPcGqiO5iDsZz0UT3xzmJq3foSCpFscyldveFaHlhxCHciC89FgLIagwb0oBzowXhpoPRlQrqycIr7PAN10YQ0k5l6xI1UmUfu-yHXaiIIFr5Kevvct-2WrHK-br8Tb7Y_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)

**Note** - I downloaded this [Detroit Demolition](https://data.detroitmi.gov/Government/Detroit-Demolitions/rv44-e9di) dataset to use rather than one of building permits provided by instructor since it was cleaner data and contained only the essential fields needed to label known blight locations.

### Data preparation & Tools
The greatest challenge in the provided data from Socrata is within the Location column because it concatenates all of the fields used in the geocoding process, and when address fields are included, line breaks are entered into the field and cause havoc until they are removed from the data file.
Before analysis of the data could be undertaken, all files were initially formatted using [Excel PowerQuery](https://support.office.com/en-us/article/Introduction-to-Microsoft-Power-Query-for-Excel-6E92E2F4-2079-4E1F-BAD5-89F6269CD605) for removal of aforementioned line breaks and standardization of the street number and addresses. Then the data was loaded into [FME](https://www.safe.com/fme/fme-desktop/), a powerful ETL tool, to validate and standardize the geographic coordinates and create well formatted incident and unique building files. Exploratory analysis and model creation was performed in [python notebooks](https://ipython.org/notebook.html) and [RStudio](https://www.rstudio.com/products/rstudio/).

### Buildings
In order to derive a building, within the incident files, the latitude and longitude coordinates were rounded to 3 decimal points and then each file was individually joined with the demolition data that also had its latitude and longitude coordinates rounded to the same number of decimal points. Where there was a match in each file, then that building record was also labeled as blight. Any incident record that lacked a coordinate was excluded from the final dataset. One interesting discovery is that street addresses are not consistently captured accross datasets and could not be relied on for aggregation due to variances in the same address.

### Features
Features drive the creation of predicton models because it is in their diversity that differences can be discovered that explain why one given building may be more prone to becoming blightful than another. In one of the readings for the course, [Spatial Characteristics of Housing Abandonment](https://github.com/uwescience/datasci_course_materials/capstone/blight/readings/morckel_spatial_characteristics_of_housing_abandonment.pdf), Dr. Morckel surmises that housing abandonment is a result of 3 key conditions - market conditions, gentrification and physical neglect. For this project, we are mostly focusing on the data evidence of neglect.

The first features added to the building dataset include a count of total 311 calls, crime incidents and blight violation citations for a given building. No filtering was performed on any of the incident datasets, because I didn't want to presume that calls about infrastructure or non-violent crimes are not related to a geographic inclination towards neglect.

A second set of features were added from a [Property Values dataset](https://data.detroitmi.gov/d/ug2h-q3n5) found on the [Detroit Data Portal](https://data.detroitmi.gov/) that included appraised and taxed values, sales price, tax status, and whether it had been improved at any point. It also included well formatted street address and latitude and longitude coordinates which helped to further reduce the overall building dataset since any buildings that lack features are not useful in generating prediction models.

Features that were not included in this project that would be interesting to add so that the model accounts for possible gentrification and economic conditions could include -
* [Building Permits](https://data.detroitmi.gov/Property-Parcels/Building-Permits-Summary/4pit-zggk) - alterations and other types that may signal gentrification in progress in neighborhood
* [Zillow Zestimate](http://www.zillow.com/zestimate/) for an address 
* [American Community Survey](https://www.census.gov/programs-surveys/acs/) annual estimates on income, mortgage and rental at Census tract block level

It would also be interesting to consider time as a factor and perhaps calculate incident counts at 90 day, 180 day and annual intervals before demolition.

## Models
For the prediction of whether a location is blighted or not, I used logistic regression and classification tree models to answer that questions. Logistic regression models the probability that an outcome belongs to a category. Classification trees identify feature importance and provide a visual representation of the feature path taken to form the best performing model. The models will be judged on their Accuracy and Kappa values. Kappa values evaluate whether the accuracy is due to random chance.

### Training & Test datasets
In order to estimate the accuracy of a model, one must divide the data into training and test datasets. I have allocated 75% of the data for training the models. I have also set both blight and IsImproved fields to factors and dropped fields from the dataset like Address, Latitude, Longitude, Parcel Number, Tax Status and Year that Residence was built so that the models evaluated only the fields - IsImproved, Appraised Value, Taxed Value, Ward Number, and number of 311 calls, crimes, and blight violations as indicators of blight. Originally, Tax Status was included, but its importance to all models was not significant. 

```{r data, warning = FALSE, message = FALSE, echo=FALSE}
setwd('~/projects/blight')
data = read.csv('building_blight_features.csv',
                  header = TRUE)
# set categorical values
data$blight = factor(data$blight)
data$IsImproved = factor(data$IsImproved)

data = na.omit(data)
# summary(data)
# select the training observations
in_train = createDataPartition(y = data$blight,
                               p = 0.75, # 75% in train, 25% in test
                               list = FALSE)

train = data[in_train, ]
test = data[-in_train, ]

# drop unuseful fields
train$Address = NULL
test$Address = NULL
train$Latitude = NULL
test$Latitude = NULL
train$Longitude = NULL
test$Longitude = NULL
train$ParcelNo = NULL
test$ParcelNo = NULL
train$PropAddr = NULL
test$PropAddr = NULL
train$ResYrBuilt = NULL
test$ResYrBuilt = NULL
train$TaxStatus = NULL
test$TaxStatus = NULL
```

### Logistic Regression Model
```{r logModel, message=FALSE, warning=FALSE, echo=FALSE,eval=TRUE}
logistic_model = train(blight ~ ., 
                       data = na.omit(train),  
                       method = 'glm',
                       family = binomial,
                       preProcess = c('center', 'scale'))

summary(logistic_model)
plot(varImp(logistic_model))

# test predictions against test set
logistic_predictions = predict(logistic_model, newdata = test)
log_matrix = confusionMatrix(logistic_predictions, test$blight)
log_matrix
```

#### Logistic Model Analysis
This model indicates that the top 3 fields that have the most predictive value are **Blight Violation Count**, **Crime Count** and **311 Call Count**. The overall accuracy of the model is 100% with a Kappa value of 100%.

### Tree Model
The first model is a simple tree that reveals the decisions that most frequently indicated what factors led to blight classfication. 
```{r treeModel, message=FALSE, warning=FALSE, echo=FALSE,eval=TRUE}
tree_model = train(factor(blight) ~., 
                   method = 'rpart',
                   data = train)
print(tree_model)
print(tree_model$finalModel)
plot(varImp(tree_model))

# plot the tree model
plot(tree_model$finalModel)
text(tree_model$finalModel, use.n = TRUE, all = TRUE, cex = 0.60)

# test predictions against test set
tree_predictions = predict(tree_model, newdata = test)
tree_matrix = confusionMatrix(tree_predictions, test$blight)
tree_matrix
```

#### Tree Model Analysis
This model indicates that the top 3 fields that have the most predictive value are **Crime Count**, **Blight Violation Count** and **311 Call Count**. The overall accuracy of the model is 99.98% with a Kappa value of 99.72%.

### Bagging Model
The next model involves Bootstrap Aggregating where the data is randomly resampled multiple times and the average is returned.
```{r bagModel, message=FALSE, warning=FALSE, echo=FALSE, eval=TRUE}
bagged_model = train(blight ~.,
                     method = 'treebag',
                     data = train)
print(bagged_model)
print(bagged_model$finalModel)
plot(varImp(bagged_model))

bagged_predictions = predict(bagged_model, test)
bag_matrix = confusionMatrix(bagged_predictions, test$blight)
bag_matrix
```

#### Bagging Model Analysis
This model indicates that the top 3 fields that have the most predictive value are **Blight Violation Count**, **Crime Count** and **311 Call Count**. The overall accuracy of the model is 100% with a Kappa value of 100%.
### Boosting Model
This model will combine weak classifiers so they can contribute to creating a more powerful model.
```{r boostModel, message=FALSE, warning=FALSE, echo=FALSE, eval=TRUE}
boost_model = train(blight ~.,
                    method = 'gbm',
                    data = train,
                    verbose = FALSE)
print(boost_model)
plot(boost_model)
summary(boost_model$finalModel)

# predict
boost_predictions = predict(boost_model, test)
boost_matrix = confusionMatrix(boost_predictions, test$blight)
boost_matrix
```

#### Boosting Model Analysis
This model indicates that the top 3 fields that have the most predictive value are **Blight Violation Count**, **Crime Count** and **311 Call Count**. The overall accuracy of the model is 100% with a Kappa value of 100%.

### Other Models
A Random Forest model is the most powerful of the tree classification models that involves bagging where it also resamples the feature combinations. However, the complexity of its algorithm causes long processing and in this case caused memory exceptions on 2 different and otherwise powerful macbookpro and imac computers, so was unable to run this model successfully.

## Model Comparison
It is easy to compare the 4 models in R and generating a visualization on how they compare using Caret package [resample methods] (http://www.inside-r.org/packages/cran/caret/docs/as.matrix.resamples).

```{r results, message=FALSE, echo=FALSE, warning=FALSE, eval=TRUE}
# compare
results = resamples(list(tree_model = tree_model, 
                         bagged_model = bagged_model,
                         boost_model = boost_model,
                         logistic_model = logistic_model))

# compare accuracy and kappa
summary(results)

# plot results
dotplot(results)
```

## Conclusion
Three of the the four models returned accuracy and kappa values of 100%, while the 4th scored 99.9% accuracy and 97% kappa values and all indicated that blight violation and crime counts are key factors in whether a building is blight, with 311 call counts also being a significant indicator. It isn't surprising that these 3 attributes are excellent indicators of blight, but their usefulness in building a model to predict the probability of blight can be questionable if the goal of a city is to become aware of potential blight early enough to be able to counter whatever conditions are truly the underlying cause of blight. For this reason, I think it is very important to build models using economic and gentrifying features in addition to physical neglect and crime statistics because they may be able to reveal problem areas early. I would also like to account for time in models and see if there is a significant difference in incident counts at different time intervals.

In addition to adding more economic features in model building, I am excited about the possibility of a new feature that FME has added to its latest release is the [RCaller transformer](http://docs.safe.com/fme/2016.1/html/FME_Desktop_Documentation/FME_Transformers/Transformers/rcaller.htm) that I hope will allow it to leverage the [Caret library](http://topepo.github.io/caret/index.html) which is what I used entirely for my models. This would make it possibly to create a workflow that could analyze the data for any city, provided the datasets conform to a general schema for 311 calls, crime incidents and blight violation counts, on a regular schedule and predict what properties are likely to be blight. If the data can indeed be standardized across a group of cities, and more known blight can be used to train the models, this would be an amazing opportunity to empower cities to proactively address potential issues while conditions are manageable rather than only be able react to them with bulldozers.

## Resources & Readings
* [Detroit Blight Removal Task Force Plan](http://report.timetoendblight.org/index.html)
* [Course Readings](https://github.com/uwescience/datasci_course_materials/tree/master/capstone/blight/readings)